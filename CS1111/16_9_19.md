# Eight great ideas in Computer Architecture
### Moore's Law
- The one constant for computer designers is rapid change, which is driven largely by Moor's Law. It states that integrated circuit resources double every 18-24 months.
- Moore's Law resulted from a 1965 prediction of such growth in IC capacity made by Moore, one of the founder of Intel.
- As computer designs can take years, the resources available per chip can easily double or quadruple between the start and finish of the project. Computer architects must anticipate where the technology will be when the design finishes rather than design for where it starts.

### Use Abstraction to Simplify Design
- Both computer architects and programmers had to invent techniques to make themselves more productive, otherwise time would lengthen as dramatically as resources grew by Moore's Law.
- A major productivity techniques for hardware and software is to use abstractions to represent the design at different levels of representation; lower-level details are hidden at higher levels

### Amdahl's Law
- This will tend to enhance performance better than optimising the rare case.
- Ironically, the common case is often simpler than the rare case and hence is often easier to enhance. This common sense advice implies that you know what the common case is, which is only possible with careful experimentation and measurement

### Performance via Parallelism 
- Since the dawn of computing, computer architects have offered designs that get more performance by performing operations in parallel

### Performance via Pipelining
- A particular pattern of parallelism is so prevalent in computer architecture that it merits its own name, pipelining
- For example, before fire engines, a "bucket brigade" would respond to a fire. A human chain was used to carry a water source to the fire

### Performance via Prediction
- Sometimes it can be better to ask for forgiveness than to ask for permission
- In some cases it can be faster on average to guess and start working rather than wait until you know for sure, assuming that the mechanism to recover from a misprediction is not too expensive and your prediction is relatively accurate

### Hierarchy of Memory
- Programmers want memory to be fast, large and cheap , as memory speed often shapes performance, capacity limits the size of problems that can be solved and the cost of memory today is often the major of computer cost
- Architects have found that they can address these conflicting demands with a hierarchy of memories, with the fastest, smallest and most expensive memory at the top and slower, larger and cheaper memory at the bottom.

### Dependability via Redundancy
- Computers not only need to be fast; they need to be dependable
- Since any physical device can fail, we make systems dependable by including redundant components that can take over when a failure occurs and to help detect failures

## Computer Organisation 
- The CPU is the engine of the computer, its function is to execute programs stored in the main memory by fetching their instructions, decoding them, and executing them one after another
- The components of the computer are connected by a bus
- A bus is a collection of parallel wires for carrying addess, data and control signals. They can be external to the CPU, connecting it to memory and I/O devices, or in the CPU. Modern computers have multiple buses